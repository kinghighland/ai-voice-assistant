#!/usr/bin/env python3
"""
GPUçŠ¶æ€æ£€æµ‹è„šæœ¬
"""

import subprocess
import sys

def test_nvidia_driver():
    """æµ‹è¯•NVIDIAé©±åŠ¨"""
    print("ğŸ” æ£€æµ‹NVIDIAé©±åŠ¨...")
    try:
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, timeout=10)
        if result.returncode == 0:
            print("âœ… NVIDIAé©±åŠ¨æ­£å¸¸")
            lines = result.stdout.split('\n')
            for line in lines:
                if 'Driver Version' in line:
                    print(f"ğŸ“‹ {line.strip()}")
                elif 'CUDA Version' in line:
                    print(f"ğŸ“‹ {line.strip()}")
            return True
        else:
            print("âŒ NVIDIAé©±åŠ¨å¼‚å¸¸")
            print(result.stderr)
            return False
    except FileNotFoundError:
        print("âŒ æœªæ‰¾åˆ°nvidia-smiå‘½ä»¤")
        print("ğŸ’¡ å¯èƒ½åŸå› : æœªå®‰è£…NVIDIAé©±åŠ¨æˆ–ä¸åœ¨PATHä¸­")
        return False
    except Exception as e:
        print(f"âŒ æ£€æµ‹å¤±è´¥: {e}")
        return False

def test_pytorch():
    """æµ‹è¯•PyTorch GPUæ”¯æŒ"""
    print("\nğŸ” æ£€æµ‹PyTorch GPUæ”¯æŒ...")
    try:
        import torch
        print(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
        
        # æ£€æŸ¥CUDAç¼–è¯‘æ”¯æŒ
        if torch.version.cuda:
            print(f"âœ… CUDAç¼–è¯‘ç‰ˆæœ¬: {torch.version.cuda}")
        else:
            print("âŒ PyTorchæœªç¼–è¯‘CUDAæ”¯æŒ")
            return False
        
        # æ£€æŸ¥CUDAè¿è¡Œæ—¶å¯ç”¨æ€§
        if torch.cuda.is_available():
            print("âœ… CUDAè¿è¡Œæ—¶å¯ç”¨")
            print(f"ğŸ”¢ GPUæ•°é‡: {torch.cuda.device_count()}")
            
            for i in range(torch.cuda.device_count()):
                props = torch.cuda.get_device_properties(i)
                print(f"ğŸ® GPU {i}: {props.name}")
                print(f"ğŸ’¾ æ˜¾å­˜: {props.total_memory / 1024**3:.1f} GB")
                print(f"ğŸ”§ è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
            
            return True
        else:
            print("âŒ CUDAè¿è¡Œæ—¶ä¸å¯ç”¨")
            return False
            
    except ImportError:
        print("âŒ æœªå®‰è£…PyTorch")
        return False
    except Exception as e:
        print(f"âŒ æ£€æµ‹å¤±è´¥: {e}")
        return False

def test_whisper_gpu():
    """æµ‹è¯•Whisper GPUæ”¯æŒ"""
    print("\nğŸ” æµ‹è¯•Whisper GPUåŠ è½½...")
    try:
        import whisper
        import torch
        
        if not torch.cuda.is_available():
            print("âš ï¸ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡GPUæµ‹è¯•")
            return False
        
        print("ğŸ“¦ å°è¯•åŠ è½½å°æ¨¡å‹åˆ°GPU...")
        device = "cuda"
        model = whisper.load_model("tiny", device=device)
        print(f"âœ… æˆåŠŸåŠ è½½Whisperæ¨¡å‹åˆ°GPU")
        
        # æµ‹è¯•GPUå†…å­˜ä½¿ç”¨
        if torch.cuda.is_available():
            memory_allocated = torch.cuda.memory_allocated() / 1024**2
            memory_reserved = torch.cuda.memory_reserved() / 1024**2
            print(f"ğŸ“Š GPUå†…å­˜ä½¿ç”¨: {memory_allocated:.1f} MB (å·²åˆ†é…)")
            print(f"ğŸ“Š GPUå†…å­˜é¢„ç•™: {memory_reserved:.1f} MB (å·²é¢„ç•™)")
        
        return True
        
    except Exception as e:
        print(f"âŒ Whisper GPUæµ‹è¯•å¤±è´¥: {e}")
        return False

def provide_solutions():
    """æä¾›è§£å†³æ–¹æ¡ˆ"""
    print("\n" + "="*50)
    print("ğŸ’¡ GPUé—®é¢˜è§£å†³æ–¹æ¡ˆ")
    print("="*50)
    
    print("\n1ï¸âƒ£ å¦‚æœNVIDIAé©±åŠ¨æœ‰é—®é¢˜:")
    print("   - è®¿é—® https://www.nvidia.com/drivers")
    print("   - ä¸‹è½½å¹¶å®‰è£…æœ€æ–°é©±åŠ¨")
    print("   - é‡å¯ç”µè„‘")
    
    print("\n2ï¸âƒ£ å¦‚æœPyTorchæ²¡æœ‰CUDAæ”¯æŒ:")
    print("   - å¸è½½ç°æœ‰PyTorch:")
    print("     pip uninstall torch torchvision torchaudio")
    print("   - å®‰è£…CUDAç‰ˆæœ¬:")
    print("     pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
    
    print("\n3ï¸âƒ£ å¦‚æœCUDAç‰ˆæœ¬ä¸åŒ¹é…:")
    print("   - æ£€æŸ¥NVIDIAé©±åŠ¨æ”¯æŒçš„CUDAç‰ˆæœ¬")
    print("   - å®‰è£…å¯¹åº”ç‰ˆæœ¬çš„PyTorch")
    print("   - å‚è€ƒ: https://pytorch.org/get-started/locally/")
    
    print("\n4ï¸âƒ£ å¦‚æœä»æœ‰é—®é¢˜:")
    print("   - æ£€æŸ¥Windowsç‰ˆæœ¬æ˜¯å¦æ”¯æŒCUDA")
    print("   - ç¡®è®¤GPUå‹å·æ”¯æŒCUDA")
    print("   - è€ƒè™‘ä½¿ç”¨CPUæ¨¡å¼ (æ€§èƒ½è¾ƒæ…¢ä½†åŠŸèƒ½å®Œæ•´)")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ GPUçŠ¶æ€è¯Šæ–­å·¥å…·")
    print("="*50)
    
    # æµ‹è¯•NVIDIAé©±åŠ¨
    nvidia_ok = test_nvidia_driver()
    
    # æµ‹è¯•PyTorch
    pytorch_ok = test_pytorch()
    
    # æµ‹è¯•Whisper
    whisper_ok = test_whisper_gpu() if pytorch_ok else False
    
    # æ€»ç»“
    print("\n" + "="*50)
    print("ğŸ“‹ è¯Šæ–­ç»“æœæ€»ç»“")
    print("="*50)
    print(f"NVIDIAé©±åŠ¨: {'âœ… æ­£å¸¸' if nvidia_ok else 'âŒ å¼‚å¸¸'}")
    print(f"PyTorch GPU: {'âœ… æ­£å¸¸' if pytorch_ok else 'âŒ å¼‚å¸¸'}")
    print(f"Whisper GPU: {'âœ… æ­£å¸¸' if whisper_ok else 'âŒ å¼‚å¸¸'}")
    
    if nvidia_ok and pytorch_ok and whisper_ok:
        print("\nğŸ‰ GPUç¯å¢ƒå®Œå…¨æ­£å¸¸ï¼å¯ä»¥ä½¿ç”¨GPUåŠ é€Ÿ")
    elif nvidia_ok and pytorch_ok:
        print("\nâš ï¸ GPUåŸºç¡€ç¯å¢ƒæ­£å¸¸ï¼Œä½†Whisper GPUåŠ è½½æœ‰é—®é¢˜")
    elif nvidia_ok:
        print("\nâš ï¸ NVIDIAé©±åŠ¨æ­£å¸¸ï¼Œä½†PyTorchæ²¡æœ‰GPUæ”¯æŒ")
    else:
        print("\nâŒ GPUç¯å¢ƒæœ‰é—®é¢˜ï¼Œå»ºè®®ä½¿ç”¨CPUæ¨¡å¼")
    
    # æä¾›è§£å†³æ–¹æ¡ˆ
    if not (nvidia_ok and pytorch_ok and whisper_ok):
        provide_solutions()

if __name__ == "__main__":
    main()