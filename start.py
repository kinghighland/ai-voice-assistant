#!/usr/bin/env python3
"""
GPUå†…å­˜ä¼˜åŒ–ç‰ˆè¯­éŸ³åŠ©æ‰‹å¯åŠ¨å™¨
- æ”¯æŒModelScopeå¿«é€Ÿä¸‹è½½
- ä¼˜å…ˆä½¿ç”¨turboæ¨¡å‹èŠ‚çœæ˜¾å­˜
- GPUå†…å­˜ç®¡ç†ä¼˜åŒ–
"""

import os
import sys
import subprocess
import time
import requests
from pathlib import Path

def check_dependencies():
    """æ£€æŸ¥ä¾èµ–"""
    print("ğŸ” æ£€æŸ¥ç³»ç»Ÿä¾èµ–...")
    
    # æ£€æŸ¥åŸºç¡€ä¾èµ–
    required_packages = ['fastapi', 'uvicorn', 'whisper', 'torch', 'requests']
    missing_packages = []
    
    for package in required_packages:
        try:
            __import__(package)
        except ImportError:
            missing_packages.append(package)
    
    if missing_packages:
        print(f"âŒ ç¼ºå°‘ä¾èµ–åŒ…: {', '.join(missing_packages)}")
        print("è¯·è¿è¡Œ: pip install -r requirements.txt")
        return False
    
    print("âœ… åŸºç¡€ä¾èµ–æ£€æŸ¥é€šè¿‡")
    return True

def check_gpu_status():
    """æ£€æŸ¥GPUçŠ¶æ€"""
    print("ğŸ” æ£€æŸ¥GPUçŠ¶æ€...")
    
    try:
        import torch
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"ğŸ® æ£€æµ‹åˆ°GPU: {gpu_name}")
            print(f"ğŸ’¾ GPUå†…å­˜: {gpu_memory:.1f} GB")
            
            # æ£€æŸ¥å¯ç”¨å†…å­˜
            torch.cuda.empty_cache()
            allocated = torch.cuda.memory_allocated() / 1024**3
            reserved = torch.cuda.memory_reserved() / 1024**3
            free = gpu_memory - reserved
            
            print(f"ğŸ“Š å†…å­˜çŠ¶æ€: å·²ç”¨ {allocated:.1f}GB, å·²é¢„ç•™ {reserved:.1f}GB, å¯ç”¨ {free:.1f}GB")
            
            if free < 2.0:
                print("âš ï¸ GPUå†…å­˜å¯èƒ½ä¸è¶³ï¼Œå»ºè®®ä½¿ç”¨turboæ¨¡å‹")
            
            return True, gpu_memory
        else:
            print("ğŸ’» æœªæ£€æµ‹åˆ°CUDA GPUï¼Œå°†ä½¿ç”¨CPUæ¨¡å¼")
            return False, 0
    except Exception as e:
        print(f"âš ï¸ GPUæ£€æŸ¥å¤±è´¥: {e}")
        return False, 0

def check_whisper_models():
    """æ£€æŸ¥Whisperæ¨¡å‹"""
    print("ğŸ” æ£€æŸ¥Whisperæ¨¡å‹...")
    
    cache_dir = Path.home() / ".cache" / "whisper"
    models = {
        "large-v3-turbo": "æ¨è: æ˜¾å­˜å ç”¨å°‘ï¼Œé€Ÿåº¦å¿«",
        "large-v3": "æœ€ä½³è´¨é‡ï¼Œæ˜¾å­˜å ç”¨å¤§",
        "medium": "å¹³è¡¡é€‰æ‹©",
        "base": "è½»é‡çº§"
    }
    
    available_models = []
    for model_name, desc in models.items():
        model_file = cache_dir / f"{model_name}.pt"
        if model_file.exists():
            size_gb = model_file.stat().st_size / 1024**3
            print(f"âœ… {model_name}: {desc} ({size_gb:.1f}GB)")
            available_models.append(model_name)
        else:
            print(f"âŒ {model_name}: {desc} (æœªä¸‹è½½)")
    
    return available_models

def suggest_model_download(has_gpu, gpu_memory):
    """å»ºè®®æ¨¡å‹ä¸‹è½½"""
    print("\nğŸ’¡ æ¨¡å‹ä¸‹è½½å»ºè®®:")
    
    if has_gpu and gpu_memory >= 8:
        print("ğŸ¯ æ¨è: large-v3-turbo (æœ€ä½³å¹³è¡¡)")
        print("   - æ˜¾å­˜å ç”¨: ~1.5GB")
        print("   - è¯†åˆ«è´¨é‡: ä¼˜ç§€")
        print("   - ä¸ºå…¶ä»–æ¨¡å‹é¢„ç•™å……è¶³å†…å­˜")
        recommended = "large-v3-turbo"
    elif has_gpu and gpu_memory >= 4:
        print("ğŸ¯ æ¨è: medium (é€‚ä¸­é€‰æ‹©)")
        print("   - æ˜¾å­˜å ç”¨: ~1GB")
        print("   - è¯†åˆ«è´¨é‡: è‰¯å¥½")
        recommended = "medium"
    else:
        print("ğŸ¯ æ¨è: base (è½»é‡çº§)")
        print("   - å†…å­˜å ç”¨: ~300MB")
        print("   - è¯†åˆ«è´¨é‡: åŸºç¡€")
        recommended = "base"
    
    print(f"\nä¸‹è½½æ–¹å¼:")
    print(f"1. ModelScopeå¿«é€Ÿä¸‹è½½: python download_whisper_modelscope.py")
    print(f"2. å®˜æ–¹ä¸‹è½½: å¯åŠ¨æœåŠ¡æ—¶è‡ªåŠ¨ä¸‹è½½")
    
    return recommended

def start_voice_service():
    """å¯åŠ¨è¯­éŸ³æœåŠ¡"""
    print("\nğŸš€ å¯åŠ¨GPUå†…å­˜ä¼˜åŒ–ç‰ˆè¯­éŸ³åŠ©æ‰‹...")
    print("ğŸ’¡ ç‰¹æ€§:")
    print("   - ä¼˜å…ˆä½¿ç”¨turboæ¨¡å‹èŠ‚çœæ˜¾å­˜")
    print("   - GPUå†…å­˜ç®¡ç†ä¼˜åŒ–")
    print("   - ä¼˜å…ˆæ‰§è¡ŒæŒ‡ä»¤ï¼Œé¿å…GPUå†²çª")
    print("   - æ”¯æŒä¸å…¶ä»–AIæ¨¡å‹å…±å­˜")
    
    try:
        # å¯åŠ¨ä¼˜åŒ–ç‰ˆæœåŠ¡
        process = subprocess.Popen([
            sys.executable, 
            "voice_api_server.py"
        ])
        
        print("\nâ³ æœåŠ¡å¯åŠ¨ä¸­...")
        print("ğŸ“± å¯åŠ¨å®Œæˆåå¯åœ¨æµè§ˆå™¨ä¸­ä½¿ç”¨è¯­éŸ³åŠŸèƒ½")
        print("ğŸ›‘ æŒ‰ Ctrl+C åœæ­¢æœåŠ¡")
        print()
        
        # ç­‰å¾…è¿›ç¨‹
        process.wait()
        
    except KeyboardInterrupt:
        print("\nğŸ›‘ æ­£åœ¨åœæ­¢æœåŠ¡...")
        try:
            process.terminate()
            time.sleep(2)
            if process.poll() is None:
                process.kill()
        except:
            pass
        print("âœ… æœåŠ¡å·²åœæ­¢")
        
    except Exception as e:
        print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¤ GPUå†…å­˜ä¼˜åŒ–ç‰ˆAIè¯­éŸ³åŠ©æ‰‹å¯åŠ¨å™¨")
    print("=" * 50)
    
    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        return
    
    # æ£€æŸ¥GPUçŠ¶æ€
    has_gpu, gpu_memory = check_gpu_status()
    
    # æ£€æŸ¥æ¨¡å‹
    print()
    available_models = check_whisper_models()
    
    if not available_models:
        print("\nâŒ æœªæ‰¾åˆ°ä»»ä½•Whisperæ¨¡å‹")
        recommended = suggest_model_download(has_gpu, gpu_memory)
        
        response = input(f"\næ˜¯å¦ä½¿ç”¨ModelScopeå¿«é€Ÿä¸‹è½½ {recommended} æ¨¡å‹? (Y/n): ").strip().lower()
        if response in ['', 'y', 'yes']:
            print("ğŸš€ å¯åŠ¨ModelScopeä¸‹è½½...")
            try:
                subprocess.run([sys.executable, "download_whisper_modelscope.py"], check=True)
            except subprocess.CalledProcessError:
                print("âŒ ModelScopeä¸‹è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨å®˜æ–¹ä¸‹è½½")
        else:
            print("ğŸ’¡ å°†åœ¨å¯åŠ¨æ—¶è‡ªåŠ¨ä¸‹è½½æ¨¡å‹")
    else:
        print(f"\nâœ… æ£€æµ‹åˆ° {len(available_models)} ä¸ªå¯ç”¨æ¨¡å‹")
        if "large-v3-turbo" in available_models:
            print("ğŸ¯ å°†ä¼˜å…ˆä½¿ç”¨ large-v3-turbo æ¨¡å‹")
        elif available_models:
            print(f"ğŸ¯ å°†ä½¿ç”¨ {available_models[0]} æ¨¡å‹")
    
    # GPUå†…å­˜ä¼˜åŒ–æç¤º
    if has_gpu:
        print(f"\nğŸ”§ GPUå†…å­˜ä¼˜åŒ–:")
        print(f"   - Whisperæ¨¡å‹å°†å ç”¨70%GPUå†…å­˜")
        print(f"   - ä¸ºå…¶ä»–AIæ¨¡å‹é¢„ç•™30%å†…å­˜")
        print(f"   - è¯­éŸ³æŒ‡ä»¤ä¼˜å…ˆæ‰§è¡Œï¼Œé¿å…æ¨¡å‹å†²çª")
    
    # å¯åŠ¨ç¡®è®¤
    print(f"\n" + "=" * 50)
    response = input("æ˜¯å¦å¼€å§‹å¯åŠ¨æœåŠ¡? (Y/n): ").strip().lower()
    if response in ['', 'y', 'yes']:
        start_voice_service()
    else:
        print("ğŸ‘‹ å¯åŠ¨å·²å–æ¶ˆ")

if __name__ == "__main__":
    main()