#!/usr/bin/env python3
"""
ä¼˜åŒ–ç‰ˆè¯­éŸ³åŠ©æ‰‹APIæœåŠ¡å™¨ - GPUå†…å­˜ä¼˜åŒ–ç‰ˆ
- æ”¯æŒWhisper-large-v3-turboæ¨¡å‹ï¼Œæ˜¾å­˜å ç”¨æ›´å°‘
- ä¼˜åŒ–GPUå†…å­˜ç®¡ç†ï¼Œæ”¯æŒå¤šæ¨¡å‹å…±å­˜
- ä¼˜å…ˆæ‰§è¡ŒæŒ‡ä»¤ï¼Œè·³è¿‡AIå›å¤é¿å…GPUå†²çª
- é›†æˆModelScopeå¿«é€Ÿä¸‹è½½
"""

from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import whisper
import requests
import os
import platform
import subprocess
import tempfile
import uvicorn
from pydantic import BaseModel
from typing import Optional, List
import logging
import torch
import re
from pathlib import Path

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# FastAPIåº”ç”¨å°†åœ¨åé¢å®šä¹‰ï¼Œé¿å…é‡å¤

# å…¨å±€å˜é‡
whisper_model = None
OLLAMA_API_BASE = "http://localhost:11434/api"

class VoiceRequest(BaseModel):
    text: str
    execute_commands: bool = True

class VoiceResponse(BaseModel):
    transcribed_text: str
    ai_response: str
    command_executed: bool = False
    command_result: Optional[str] = None
    command_type: Optional[str] = None

# æ‰©å±•çš„æŒ‡ä»¤è¯†åˆ«è¯å…¸
COMMAND_PATTERNS = {
    "åº”ç”¨ç¨‹åº": {
        "keywords": ["æ‰“å¼€", "å¯åŠ¨", "è¿è¡Œ", "å¼€å¯"],
        "targets": {
            "è®°äº‹æœ¬": ["è®°äº‹æœ¬", "notepad", "æ–‡æœ¬ç¼–è¾‘å™¨"],
            "è®¡ç®—å™¨": ["è®¡ç®—å™¨", "calculator", "è®¡æ—¶å™¨", "è®¡æ—¶ç‰ˆ", "è®¡ç®—æœº"],
            "ç”»å›¾": ["ç”»å›¾", "ç”»æ¿", "ç»˜å›¾", "paint"],
            "æ–‡ä»¶ç®¡ç†å™¨": ["æ–‡ä»¶ç®¡ç†å™¨", "èµ„æºç®¡ç†å™¨", "æ–‡ä»¶å¤¹", "explorer"],
            "æµè§ˆå™¨": ["æµè§ˆå™¨", "browser", "ç½‘é¡µ", "ä¸Šç½‘"],
            "ä»»åŠ¡ç®¡ç†å™¨": ["ä»»åŠ¡ç®¡ç†å™¨", "è¿›ç¨‹ç®¡ç†", "task manager"],
            "æ§åˆ¶é¢æ¿": ["æ§åˆ¶é¢æ¿", "è®¾ç½®", "ç³»ç»Ÿè®¾ç½®"],
            "å‘½ä»¤æç¤ºç¬¦": ["å‘½ä»¤æç¤ºç¬¦", "cmd", "ç»ˆç«¯", "æ§åˆ¶å°"],
            "PowerShell": ["powershell", "ps", "power shell"]
        }
    },
    "ç½‘ç«™": {
        "keywords": ["æ‰“å¼€", "è®¿é—®", "è¿›å…¥", "å»", "çœ‹çœ‹"],
        "targets": {
            "ç™¾åº¦": ["ç™¾åº¦", "baidu"],
            "è°·æ­Œ": ["è°·æ­Œ", "google", "æœç´¢"],
            "çŸ¥ä¹": ["çŸ¥ä¹", "zhihu"],
            "å¾®åš": ["å¾®åš", "weibo"],
            "å“”å“©å“”å“©": ["å“”å“©å“”å“©", "bilibili", "bç«™", "Bç«™"],
            "æ·˜å®": ["æ·˜å®", "taobao", "è´­ç‰©"],
            "äº¬ä¸œ": ["äº¬ä¸œ", "jd", "å•†åŸ"],
            "GitHub": ["github", "ä»£ç ", "å¼€æº"],
            "YouTube": ["youtube", "æ²¹ç®¡", "è§†é¢‘"],
            "ç½‘æ˜“äº‘éŸ³ä¹": ["ç½‘æ˜“äº‘", "éŸ³ä¹", "æ­Œæ›²"]
        }
    },
    "ç³»ç»Ÿæ“ä½œ": {
        "keywords": ["å…³é—­", "é€€å‡º", "ç»“æŸ", "åœæ­¢", "é‡å¯", "å…³æœº"],
        "targets": {
            "å…³æœº": ["å…³æœº", "shutdown", "å…³é—­ç”µè„‘"],
            "é‡å¯": ["é‡å¯", "restart", "é‡æ–°å¯åŠ¨"],
            "æ³¨é”€": ["æ³¨é”€", "logout", "ç™»å‡º"],
            "é”å±": ["é”å±", "lock", "é”å®šå±å¹•"],
            "ä¼‘çœ ": ["ä¼‘çœ ", "sleep", "å¾…æœº"]
        }
    },
    "æ–‡ä»¶æ“ä½œ": {
        "keywords": ["æ–°å»º", "åˆ›å»º", "åˆ é™¤", "å¤åˆ¶", "ç§»åŠ¨"],
        "targets": {
            "æ–°å»ºæ–‡ä»¶å¤¹": ["æ–°å»ºæ–‡ä»¶å¤¹", "åˆ›å»ºæ–‡ä»¶å¤¹", "å»ºæ–‡ä»¶å¤¹"],
            "æ–°å»ºæ–‡ä»¶": ["æ–°å»ºæ–‡ä»¶", "åˆ›å»ºæ–‡ä»¶", "å»ºæ–‡ä»¶"],
            "æˆªå›¾": ["æˆªå›¾", "æˆªå±", "æŠ“å›¾", "screenshot"]
        }
    }
}

from contextlib import asynccontextmanager



@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶æ‰§è¡Œ
    global whisper_model
    logger.info("ğŸš€ æ­£åœ¨å¯åŠ¨ä¼˜åŒ–ç‰ˆè¯­éŸ³åŠ©æ‰‹APIæœåŠ¡...")
    
    logger.info("ğŸ“¥ å¼€å§‹åŠ è½½Whisperæ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œå¯èƒ½éœ€è¦ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼‰...")
    
    # å¯¼å…¥å¿…è¦çš„æ¨¡å—
    import torch
    import whisper
    import gc
    
    # æ£€æŸ¥GPUå¯ç”¨æ€§
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"ğŸ”§ ä½¿ç”¨è®¾å¤‡: {device}")
    
    if device == "cuda":
        logger.info(f"ğŸ® GPUä¿¡æ¯: {torch.cuda.get_device_name(0)}")
        logger.info(f"ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # ä¼˜å…ˆä½¿ç”¨turboæ¨¡å‹ä»¥èŠ‚çœGPUå†…å­˜
    model_priority = ["large-v3-turbo", "large-v3", "large-v2", "medium", "base"]
    
    for i, model_name in enumerate(model_priority):
        try:
            logger.info(f"ğŸ“¦ å°è¯•åŠ è½½ Whisper {model_name} æ¨¡å‹... ({i+1}/{len(model_priority)})")
            
            # æä¾›æ¨¡å‹å¤§å°ä¿¡æ¯
            model_sizes = {
                "large-v3-turbo": "1.5GB",  # turboç‰ˆæœ¬æ˜¾å­˜å ç”¨æ›´å°‘
                "large-v3": "3.1GB",
                "large-v2": "3.1GB", 
                "large": "3.1GB",
                "medium": "1.5GB",
                "base": "290MB"
            }
            
            if model_name in model_sizes:
                logger.info(f"ğŸ“Š æ¨¡å‹å¤§å°: {model_sizes[model_name]}")
                if i == 0:
                    logger.info("ğŸ’¡ æç¤º: é¦–æ¬¡ä¸‹è½½å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œæ¨¡å‹ä¼šç¼“å­˜åˆ°æœ¬åœ°")
                    logger.info("ğŸŒ ä½¿ç”¨å®˜æ–¹æºä¸‹è½½ (å¯èƒ½è¾ƒæ…¢ï¼Œè¯·è€å¿ƒç­‰å¾…)")
            
            # æ˜¾ç¤ºåŠ è½½çŠ¶æ€
            logger.info("â³ æ­£åœ¨åŠ è½½æ¨¡å‹ï¼Œè¯·ç¨å€™...")
            if i == 0:
                logger.info("ğŸ’¡ é¦–æ¬¡ä¸‹è½½å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...")
                logger.info("ğŸŒ å¦‚æœä¸‹è½½å¾ˆæ…¢ï¼Œå¯ä»¥æŒ‰ Ctrl+C ä¸­æ–­ï¼Œç¨åé‡è¯•")
            
            # æ¸…ç†GPUå†…å­˜
            if device == "cuda":
                torch.cuda.empty_cache()
                gc.collect()
            
            # ç‰¹æ®Šå¤„ç†turboæ¨¡å‹ - ç›´æ¥ä»æ–‡ä»¶åŠ è½½
            if model_name == "large-v3-turbo":
                turbo_path = Path.home() / ".cache" / "whisper" / "large-v3-turbo.pt"
                if turbo_path.exists():
                    logger.info(f"ğŸ¯ ç›´æ¥åŠ è½½turboæ¨¡å‹æ–‡ä»¶: {turbo_path}")
                    whisper_model = whisper.load_model(str(turbo_path), device=device)
                else:
                    logger.warning(f"âš ï¸ turboæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {turbo_path}")
                    continue
            else:
                # åŠ è½½å®˜æ–¹æ¨¡å‹
                whisper_model = whisper.load_model(
                    model_name, 
                    device=device,
                    in_memory=True  # ä¿æŒåœ¨å†…å­˜ä¸­ä»¥æé«˜æ€§èƒ½
                )
            
            # å¦‚æœæ˜¯GPUæ¨¡å¼ï¼Œè®¾ç½®å†…å­˜åˆ†é…ç­–ç•¥
            if device == "cuda":
                # è®¾ç½®æ›´ä¿å®ˆçš„å†…å­˜åˆ†é…ï¼Œä¸ºå…¶ä»–æ¨¡å‹é¢„ç•™æ›´å¤šç©ºé—´
                torch.cuda.set_per_process_memory_fraction(0.5)  # åªä½¿ç”¨50%GPUå†…å­˜
                logger.info("ğŸ”§ GPUå†…å­˜åˆ†é…: 50% (ä¸ºå…¶ä»–AIæ¨¡å‹é¢„ç•™50%)")
            
            logger.info(f"âœ… æˆåŠŸåŠ è½½ Whisper {model_name} æ¨¡å‹åˆ° {device}")
            
            # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
            if hasattr(whisper_model, 'dims'):
                logger.info(f"ğŸ”§ æ¨¡å‹å‚æ•°: {whisper_model.dims}")
            
            break
        except Exception as e:
            logger.warning(f"âŒ æ— æ³•åŠ è½½ {model_name} æ¨¡å‹: {e}")
            if i < len(model_priority) - 1:
                logger.info(f"ğŸ”„ å°è¯•åŠ è½½ä¸‹ä¸€ä¸ªæ¨¡å‹...")
            continue
    
    if whisper_model is None:
        logger.error("ğŸ’¥ æ‰€æœ‰æ¨¡å‹åŠ è½½å¤±è´¥ï¼")
        raise RuntimeError("æ— æ³•åŠ è½½ä»»ä½•Whisperæ¨¡å‹")
    
    logger.info("ğŸ‰ è¯­éŸ³åŠ©æ‰‹APIæœåŠ¡å¯åŠ¨å®Œæˆï¼")
    logger.info(f"ğŸŒ æœåŠ¡åœ°å€: http://localhost:8889")
    logger.info(f"ğŸ“š APIæ–‡æ¡£: http://localhost:8889/docs")
    
    yield  # åº”ç”¨è¿è¡ŒæœŸé—´
    
    # å…³é—­æ—¶æ‰§è¡Œ
    logger.info("ğŸ›‘ æ­£åœ¨å…³é—­è¯­éŸ³åŠ©æ‰‹APIæœåŠ¡...")

# é‡æ–°åˆ›å»ºFastAPIåº”ç”¨ï¼Œæ­£ç¡®è®¾ç½®lifespanå‚æ•°
app = FastAPI(
    title="ä¼˜åŒ–ç‰ˆè¯­éŸ³åŠ©æ‰‹API", 
    version="2.0.0",
    description="æ”¯æŒGPUåŠ é€Ÿçš„ä¸­æ–‡è¯­éŸ³è¯†åˆ«å’Œæ™ºèƒ½æŒ‡ä»¤æ‰§è¡Œ",
    lifespan=lifespan
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    return {"message": "ä¼˜åŒ–ç‰ˆè¯­éŸ³åŠ©æ‰‹APIæœåŠ¡æ­£åœ¨è¿è¡Œ", "status": "healthy"}

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    device_info = "GPU" if torch.cuda.is_available() else "CPU"
    return {
        "status": "healthy", 
        "whisper_loaded": whisper_model is not None,
        "device": device_info,
        "model_info": str(whisper_model) if whisper_model else None
    }

def preprocess_chinese_text(text: str) -> str:
    """é¢„å¤„ç†ä¸­æ–‡æ–‡æœ¬ï¼Œä¿®æ­£å¸¸è§è¯†åˆ«é”™è¯¯"""
    corrections = {
        # å¸¸è§çš„ä¸­æ–‡è¯†åˆ«é”™è¯¯ä¿®æ­£
        "è®¡æ—¶ç‰ˆ": "è®°äº‹æœ¬",
        "è®¡æ—¶å™¨": "è®¡ç®—å™¨", 
        "è®¡ç®—æœº": "è®¡ç®—å™¨",
        "è®°äº‹ç‰ˆ": "è®°äº‹æœ¬",
        "æ–‡ä»¶ç®¡ç†": "æ–‡ä»¶ç®¡ç†å™¨",
        "èµ„æºç®¡ç†": "æ–‡ä»¶ç®¡ç†å™¨",
        "ä»»åŠ¡ç®¡ç†": "ä»»åŠ¡ç®¡ç†å™¨",
        "æ§åˆ¶ç‰ˆ": "æ§åˆ¶é¢æ¿",
        "å“”å“©å“”å“©": "bilibili",
        "Bç«™": "bilibili",
        "bç«™": "bilibili",
        "ç½‘æ˜“äº‘": "ç½‘æ˜“äº‘éŸ³ä¹",
        "è°·æ­Œ": "google",
        "ç™¾åº¦ä¸€ä¸‹": "ç™¾åº¦",
        # æ–°å¢çš„å¸¸è§é”™è¯¯
        "å¤§å¼€": "æ‰“å¼€",
        "æ‰“å¼€æµè§ˆå™¨è®¿é—®": "æ‰“å¼€",
        "æµè§ˆå™¨è®¿é—®": "æ‰“å¼€",
        "è®¿é—®çŸ¥ä¹ç½‘ç«™": "çŸ¥ä¹",
        "çŸ¥ä¹ç½‘ç«™": "çŸ¥ä¹",
        "ç™¾åº¦ç½‘ç«™": "ç™¾åº¦",
        "è°·æ­Œç½‘ç«™": "è°·æ­Œ",
        "å¾®åšç½‘ç«™": "å¾®åš",
        "è®¡ç®—æœºå™¨": "è®¡ç®—å™¨",
        "è®°äº‹ç°¿": "è®°äº‹æœ¬",
        "æ–‡æœ¬ç¼–è¾‘": "è®°äº‹æœ¬"
    }
    
    corrected_text = text
    for wrong, correct in corrections.items():
        corrected_text = corrected_text.replace(wrong, correct)
    
    return corrected_text

def smart_command_detection(text: str) -> tuple[bool, str, str]:
    """æ™ºèƒ½æŒ‡ä»¤æ£€æµ‹ - è¿”å›(æ˜¯å¦ä¸ºæŒ‡ä»¤, æŒ‡ä»¤ç±»å‹, ç›®æ ‡)"""
    text = preprocess_chinese_text(text.strip())
    text_lower = text.lower()
    
    # æ£€æŸ¥æ¯ç§æŒ‡ä»¤ç±»å‹
    for cmd_type, config in COMMAND_PATTERNS.items():
        keywords = config["keywords"]
        targets = config["targets"]
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«å…³é”®è¯
        has_keyword = any(keyword in text for keyword in keywords)
        
        if has_keyword:
            # æ£€æŸ¥ç›®æ ‡
            for target_name, target_aliases in targets.items():
                if any(alias in text_lower for alias in target_aliases):
                    return True, cmd_type, target_name
    
    # ç‰¹æ®Šæƒ…å†µï¼šç›´æ¥è¯´åº”ç”¨åç§°
    for cmd_type, config in COMMAND_PATTERNS.items():
        if cmd_type in ["åº”ç”¨ç¨‹åº", "ç½‘ç«™"]:
            for target_name, target_aliases in config["targets"].items():
                if any(alias in text_lower for alias in target_aliases):
                    # å¦‚æœæ–‡æœ¬å¾ˆçŸ­ä¸”ä¸»è¦æ˜¯åº”ç”¨åç§°ï¼Œè®¤ä¸ºæ˜¯æŒ‡ä»¤
                    if len(text.strip()) <= 10:
                        return True, cmd_type, target_name
    
    return False, "", ""

@app.post("/transcribe", response_model=dict)
async def transcribe_audio(audio_file: UploadFile = File(...)):
    """ä¼˜åŒ–çš„è¯­éŸ³è½¬æ–‡å­—æ¥å£"""
    if not whisper_model:
        raise HTTPException(status_code=500, detail="Whisperæ¨¡å‹æœªåŠ è½½")
    
    try:
        # ä¿å­˜ä¸Šä¼ çš„éŸ³é¢‘æ–‡ä»¶
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as temp_file:
            content = await audio_file.read()
            temp_file.write(content)
            temp_file_path = temp_file.name
        
        # ä¼˜åŒ–çš„Whisperè½¬å½•å‚æ•°
        logger.info("å¼€å§‹è½¬å½•éŸ³é¢‘...")
        result = whisper_model.transcribe(
            temp_file_path,
            language="zh",  # å¼ºåˆ¶ä¸­æ–‡
            initial_prompt="ä»¥ä¸‹æ˜¯æ™®é€šè¯çš„è½¬å½•ï¼Œè¯·å‡†ç¡®è¯†åˆ«åº”ç”¨ç¨‹åºåç§°å¦‚è®°äº‹æœ¬ã€è®¡ç®—å™¨ç­‰ã€‚",
            temperature=0.0,  # é™ä½éšæœºæ€§
            beam_size=5,      # å¢åŠ beam search
            best_of=5,        # å¤šæ¬¡å°è¯•å–æœ€ä½³
            fp16=torch.cuda.is_available(),  # GPUæ—¶ä½¿ç”¨fp16åŠ é€Ÿ
            condition_on_previous_text=False,  # ä¸ä¾èµ–å‰æ–‡
            no_speech_threshold=0.6,
            logprob_threshold=-1.0,
            compression_ratio_threshold=2.4
        )
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(temp_file_path)
        
        # é¢„å¤„ç†è½¬å½•ç»“æœ
        transcribed_text = preprocess_chinese_text(result["text"].strip())
        logger.info(f"è½¬å½•ç»“æœ: {transcribed_text}")
        
        # æ™ºèƒ½æŒ‡ä»¤æ£€æµ‹
        is_command, cmd_type, target = smart_command_detection(transcribed_text)
        
        return {
            "success": True,
            "transcribed_text": transcribed_text,
            "language": result.get("language", "zh"),
            "is_command": is_command,
            "command_type": cmd_type,
            "command_target": target,
            "confidence": result.get("avg_logprob", 0)
        }
        
    except Exception as e:
        logger.error(f"è½¬å½•é”™è¯¯: {str(e)}")
        if 'temp_file_path' in locals():
            try:
                os.unlink(temp_file_path)
            except:
                pass
        raise HTTPException(status_code=500, detail=f"è½¬å½•å¤±è´¥: {str(e)}")

def execute_enhanced_command(cmd_type: str, target: str, original_text: str) -> Optional[str]:
    """æ‰§è¡Œå¢å¼ºçš„ç³»ç»Ÿå‘½ä»¤"""
    system = platform.system().lower()
    
    try:
        if cmd_type == "åº”ç”¨ç¨‹åº":
            if system == 'windows':
                commands = {
                    "è®°äº‹æœ¬": "notepad",
                    "è®¡ç®—å™¨": "calc", 
                    "ç”»å›¾": "mspaint",
                    "æ–‡ä»¶ç®¡ç†å™¨": "explorer",
                    "æµè§ˆå™¨": "start msedge",
                    "ä»»åŠ¡ç®¡ç†å™¨": "taskmgr",
                    "æ§åˆ¶é¢æ¿": "control",
                    "å‘½ä»¤æç¤ºç¬¦": "cmd",
                    "PowerShell": "powershell"
                }
                
                if target in commands:
                    subprocess.run(commands[target], shell=True)
                    return f"âœ… å·²ä¸ºæ‚¨æ‰“å¼€{target}"
        
        elif cmd_type == "ç½‘ç«™":
            urls = {
                "ç™¾åº¦": "https://www.baidu.com",
                "è°·æ­Œ": "https://www.google.com", 
                "çŸ¥ä¹": "https://www.zhihu.com",
                "å¾®åš": "https://weibo.com",
                "å“”å“©å“”å“©": "https://www.bilibili.com",
                "æ·˜å®": "https://www.taobao.com",
                "äº¬ä¸œ": "https://www.jd.com",
                "GitHub": "https://github.com",
                "YouTube": "https://www.youtube.com",
                "ç½‘æ˜“äº‘éŸ³ä¹": "https://music.163.com"
            }
            
            if target in urls:
                if system == 'windows':
                    os.startfile(urls[target])
                    return f"âœ… å·²ä¸ºæ‚¨æ‰“å¼€{target}"
                else:
                    return f"æ£€æµ‹åˆ°æ‰“å¼€ç½‘ç«™å‘½ä»¤: {urls[target]}"
        
        elif cmd_type == "ç³»ç»Ÿæ“ä½œ":
            if system == 'windows':
                operations = {
                    "å…³æœº": "shutdown /s /t 0",
                    "é‡å¯": "shutdown /r /t 0", 
                    "æ³¨é”€": "shutdown /l",
                    "é”å±": "rundll32.exe user32.dll,LockWorkStation"
                }
                
                if target in operations:
                    # å±é™©æ“ä½œéœ€è¦ç¡®è®¤
                    if target in ["å…³æœº", "é‡å¯", "æ³¨é”€"]:
                        return f"âš ï¸ æ£€æµ‹åˆ°{target}å‘½ä»¤ï¼Œè¯·æ‰‹åŠ¨ç¡®è®¤æ‰§è¡Œ"
                    else:
                        subprocess.run(operations[target], shell=True)
                        return f"âœ… å·²æ‰§è¡Œ{target}"
        
        elif cmd_type == "æ–‡ä»¶æ“ä½œ":
            if target == "æˆªå›¾" and system == 'windows':
                # ä½¿ç”¨Windowsæˆªå›¾å·¥å…·
                subprocess.run("snippingtool", shell=True)
                return "âœ… å·²æ‰“å¼€æˆªå›¾å·¥å…·"
    
    except Exception as e:
        logger.error(f"æ‰§è¡Œå‘½ä»¤é”™è¯¯: {str(e)}")
        return f"âŒ å‘½ä»¤æ‰§è¡Œå¤±è´¥: {str(e)}"
    
    return None

@app.post("/process", response_model=VoiceResponse)
async def process_voice_command(request: VoiceRequest):
    """å¤„ç†è¯­éŸ³å‘½ä»¤æ¥å£"""
    try:
        text = request.text
        logger.info(f"å¤„ç†å‘½ä»¤: {text}")
        
        # æ™ºèƒ½æŒ‡ä»¤æ£€æµ‹
        is_command, cmd_type, target = smart_command_detection(text)
        logger.info(f"æŒ‡ä»¤æ£€æµ‹ç»“æœ: is_command={is_command}, cmd_type={cmd_type}, target={target}")
        
        # æ‰§è¡Œç³»ç»Ÿå‘½ä»¤ (ä¼˜å…ˆæ‰§è¡Œï¼Œä¸ä¾èµ–AIå›å¤)
        command_executed = False
        command_result = None
        
        if request.execute_commands and is_command:
            logger.info(f"å¼€å§‹æ‰§è¡ŒæŒ‡ä»¤: {cmd_type} - {target}")
            command_result = execute_enhanced_command(cmd_type, target, text)
            if command_result and not command_result.startswith("æŠ±æ­‰"):
                command_executed = True
                logger.info(f"æŒ‡ä»¤æ‰§è¡ŒæˆåŠŸ: {command_result}")
            else:
                logger.warning(f"æŒ‡ä»¤æ‰§è¡Œå¤±è´¥: {command_result}")
        
        # è·å–AIå›å¤ (åªæœ‰éæŒ‡ä»¤æ‰éœ€è¦AIå›å¤)
        ai_response = "æŒ‡ä»¤å·²å¤„ç†" if is_command else "æ­£åœ¨å¤„ç†æ‚¨çš„è¯·æ±‚..."
        
        # åªæœ‰æ™®é€šå¯¹è¯æ‰è°ƒç”¨AIæ¨¡å‹ï¼Œé¿å…GPUå†…å­˜å†²çª
        if not is_command:
            try:
                # ä¸ºäº†é¿å…GPUå†…å­˜å†²çªï¼Œæš‚æ—¶ç¦ç”¨AIå›å¤
                # ai_response = await get_ai_response(text)
                ai_response = "è¯­éŸ³æŒ‡ä»¤æ¨¡å¼ä¸‹æš‚ä¸æ”¯æŒAIå¯¹è¯ï¼Œè¯·ç›´æ¥åœ¨èŠå¤©æ¡†ä¸­è¾“å…¥æ–‡å­—è¿›è¡ŒAIå¯¹è¯"
            except Exception as e:
                logger.warning(f"AIå›å¤è·å–å¤±è´¥: {e}")
                ai_response = "æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨"
        
        return VoiceResponse(
            transcribed_text=text,
            ai_response=ai_response,
            command_executed=command_executed,
            command_result=command_result,
            command_type=cmd_type if is_command else None
        )
        
    except Exception as e:
        logger.error(f"å¤„ç†å‘½ä»¤é”™è¯¯: {str(e)}")
        raise HTTPException(status_code=500, detail=f"å¤„ç†å¤±è´¥: {str(e)}")

async def get_ai_response(text: str) -> str:
    """è·å–AIå›å¤"""
    models_to_try = ['minicpm-v:latest', 'qwen3:14b', 'deepseek-r1:14b', 'llama3.2-vision:11b']
    
    for model_name in models_to_try:
        try:
            logger.info(f"å°è¯•æ¨¡å‹: {model_name}")
            response = requests.post(
                f"{OLLAMA_API_BASE}/generate",
                json={
                    "model": model_name,
                    "prompt": f"è¯·ç”¨ä¸­æ–‡å›ç­”è¿™ä¸ªé—®é¢˜: {text}",
                    "stream": False
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                ai_response = result['response'].strip()
                logger.info(f"æˆåŠŸä½¿ç”¨æ¨¡å‹: {model_name}")
                return ai_response
            else:
                logger.warning(f"æ¨¡å‹ {model_name} HTTPé”™è¯¯: {response.status_code}")
                continue
                
        except Exception as e:
            logger.warning(f"æ¨¡å‹ {model_name} å¤±è´¥: {str(e)}")
            continue
    
    return "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•è¿æ¥åˆ°AIæ¨¡å‹ã€‚è¯·ç¡®ä¿Ollamaæ­£åœ¨è¿è¡Œå¹¶ä¸”å·²å®‰è£…æ¨¡å‹ã€‚"

if __name__ == "__main__":
    from config import Config
    
    # æ‰“å°é…ç½®ä¿¡æ¯
    Config.print_config()
    
    # ä½¿ç”¨é…ç½®å¯åŠ¨æœåŠ¡
    uvicorn.run(app, host=Config.API_HOST, port=Config.API_PORT)